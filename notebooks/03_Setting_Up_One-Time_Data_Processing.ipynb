{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a42680d-3e10-4961-9605-2ae87ffd9fc2",
   "metadata": {},
   "source": [
    "# M5.3 - Setting Up One-Time Data Processing\n",
    "\n",
    "*Part of:* [**Open Climate Science for Crops & Crop Conditions**](https://github.com/OpenClimateScience/M5-Open-Science-for-Crops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa5640c-072c-441f-81e2-14cbfd514189",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import glob\n",
    "import os\n",
    "import earthaccess\n",
    "import numpy as np\n",
    "import h5py\n",
    "import xarray as xr\n",
    "import rasterio\n",
    "import rioxarray\n",
    "import py4eos\n",
    "import pyproj\n",
    "from shapely.geometry import Polygon\n",
    "from rasterio.warp import calculate_default_transform, reproject, Resampling\n",
    "from matplotlib import pyplot\n",
    "from tqdm import tqdm\n",
    "\n",
    "auth = earthaccess.login()\n",
    "\n",
    "LC_DIR = 'data/MCD12Q1'\n",
    "VNP16_DIR = 'data/VNP16A2GF'\n",
    "IMERG_DIR = 'data/IMERG'\n",
    "OUTPUT_LC_FILENAME = 'data/processed/MODIS_MCD12Q1_Type5_cereal_croplands_h15v05_2023.tiff'\n",
    "OUTPUT_VNP16_DIR = 'data/processed/VNP16_ET_and_PET'\n",
    "OUTPUT_IMERG_DIR = 'data/processed'\n",
    "TIME_PERIOD = ('2023-10-01', '2024-09-30')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b58b207-5803-426f-8c6a-091f77814543",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "To compute the water requirement satisfaction index (WRSI), we will need the following data:\n",
    "\n",
    "- A land-cover map (to help us identify where croplands are in our study area)\n",
    "- Potential evapotranspiration (PET)\n",
    "- Precipitation\n",
    "\n",
    "For each of these, NASA has a product we can use:\n",
    "\n",
    "- [The MODIS MCD12Q1 dataset](https://doi.org/10.5067/MODIS/MCD12Q1.061) describes the land-cover class for each 500-m pixel on the MODIS sinusoidal grid.\n",
    "- As we've seen previously, [the MODIS MOD16A2 product](https://doi.org/10.5067/MODIS/MOD16A2.006) provides terrestrial evapotranspiration (ET) and PET estimates every 8 days across the entire globe.\n",
    "- And we've also previously used NASA's precipitation data from [the Global Precipitation Monitoring (GPM) mission's IMERG-Final dataset.](https://disc.gsfc.nasa.gov/datasets/GPM_3IMERGDF_06/summary)\n",
    "\n",
    "**In this lesson, we'll prepare the datasets we need for computing the WRSI. Importantly, we will also see how to write Snakemake rules that will enable future users to quickly reproduce our data processing steps.**\n",
    "\n",
    "Specifically, we want to:\n",
    "\n",
    "1. Resample and project the land-cover data onto a 1-km EASE-Grid 2.0, for easy comparison to other datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f837c1-5f1c-46c7-b861-5dde9fd6cf5b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Getting the data we need\n",
    "\n",
    "We'll continue to use the `earthaccess` library for programmatic access to NASA Earthdata Search. Recall that the `earthaccess.search_data()` function has a `bounding_box` argument that allows us to search for datasets that fall within a rectangular bounding box, defined by latitude-longitude coordinates.\n",
    "\n",
    "```python\n",
    "help(earthaccess.search_data)\n",
    "```\n",
    "\n",
    "```\n",
    "**bounding_box**: a tuple representing spatial bounds in the form\n",
    "    `(lower_left_lon, lower_left_lat, upper_right_lon, upper_right_lat)`\n",
    "```\n",
    "\n",
    "We'll define our bounding box, `bbox`, for our study area, a part of Northern Africa centered on Algeria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d6d954-2224-4484-9a46-b6dc8a9bfc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The bounding box for our study area\n",
    "bbox = (1.5, 34.0, 8.0, 37.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efb2068-4ad3-490a-baf7-f9c8755a46cf",
   "metadata": {},
   "source": [
    "### Downloading a land-cover map for our study area\n",
    "\n",
    "As we begin to download the data we're going to use, we should remember to record **the Digital Object Identifier (DOI)** for each dataset. DOIs not only allow us to properly cite the dataset and give its authors due credit; they allow others to accurately identify the dataset we used. For the MODIS MCD12Q1 land-cover data, the DOI is:\n",
    "\n",
    "- DOI: https://doi.org/10.5067/MODIS/MCD12Q1.061"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd680ce-90b0-4738-a16c-25a18b1ba04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that TIME_PERIOD refers to one of our global variables, defined at the top of the script\n",
    "results = earthaccess.search_data(\n",
    "    short_name = 'MCD12Q1',\n",
    "    temporal = TIME_PERIOD,\n",
    "    bounding_box = tuple(bbox))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158e0c0a-e321-4065-be96-d99356eb15a1",
   "metadata": {},
   "source": [
    "There are two results because our `TIME_PERIOD` spans two years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e717764-0279-4d80-b359-7db4dbae647d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842b8f13-c76a-42bf-a08d-39e7c474ef65",
   "metadata": {},
   "source": [
    "As we start to think about a reproducible data-processing pipeline, one thing we might want to avoid is having to re-download large datasets. If the data already exist, let's not download the data again. \n",
    "\n",
    "Because we were careful to create separate directories for our datasets, a simple check for the existence of files can be used to determine if we need to download any data. Recall that the `glob.glob()` function will list the contents of our directory search, where the `*` character is a wildcard matching any characters in a filename:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb693455-def4-4bfb-8c8b-40f235af94c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the directory we're searching\n",
    "print(f'{LC_DIR}/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2802cd-7c24-4619-8a69-9ad0672ed9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only download the files once; i.e., if we haven't already downloaded any\n",
    "if len(glob.glob(f'{LC_DIR}/*')) == 0:\n",
    "    earthaccess.download(results, LC_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebee4cb2-3684-42fb-8425-b16d8dd6353d",
   "metadata": {},
   "source": [
    "**We'll use the `py4eos` library, [which we saw earlier in Module 3,](https://github.com/OpenClimateScience/M3-Open-Science-for-Water-Resources) to open the MODIS MCD12Q1 land-cover dataset for 2023.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e62a764-55dd-4246-879a-3b059f06b46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf = py4eos.read_file('data/MCD12Q1/MCD12Q1.A2023001.h18v05.061.2024252125305.hdf', platform = 'MODIS')\n",
    "hdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0882754-3af6-410e-930f-730453dffc9a",
   "metadata": {},
   "source": [
    "Next, we want to get the land-cover dataset as a `rasterio` dataset so that we can easily resample it to 1-km resolution. We're using the `\"LC_Type5\"` dataset, which is just one of multiple land-cover classifications that are available in MODIS MCD12Q1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfb0145-4fdd-4306-ac9a-a1410812b628",
   "metadata": {},
   "outputs": [],
   "source": [
    "lc_raster = hdf.to_rasterio('LC_Type5', filename = '', driver = 'MEM')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080162c4-fa9a-4951-8b74-0f5e8b6931e1",
   "metadata": {},
   "source": [
    "Let's generate a preview of our land-cover map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424230ac-9053-4987-b608-2e51de1cd3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "lc_map = lc_raster.read(1)\n",
    "pyplot.imshow(lc_map, interpolation = 'nearest', vmax = 11)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75f5f95-297b-4641-8564-6c0fbd4f77be",
   "metadata": {},
   "source": [
    "### Projecting the land-cover data\n",
    "\n",
    "Next, we want to project the land-cover data onto a 1-km EASE-Grid 2.0. It can be difficult to calculate the new width and height of a raster dataset after projection, so we use [the `calculate_default_transform()` function](https://rasterio.readthedocs.io/en/latest/api/rasterio.warp.html#rasterio.warp.calculate_default_transform) in the `rasterio.warp` module to figure this out.\n",
    "\n",
    "In `calculate_default_transform()`, we need to tell it the coordinate reference system (CRS) of our dataset, the new CRS we want, the current width and height (given by `lc_raster.shape`), and the spatial bounds of the raster (given by `lc_raster.bounds`).\n",
    "\n",
    "The output from this function is a 3-element tuple, so we can assign the output to three variables. The last two are the new `width` and `height` of the projected raster. The first output, `new_transform`, contains other important information about the new, projected raster we're going to create."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8d2fbe-35a2-4a49-aeea-18dce4024ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: 6933 is the EPSG code for the global EASE-Grid 2.0 projection\n",
    "new_transform, width, height = calculate_default_transform(\n",
    "    lc_raster.crs, pyproj.CRS(6933), *lc_raster.shape, *lc_raster.bounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f9aae2-561e-44d7-9ce1-ac4c1d19d949",
   "metadata": {},
   "source": [
    "Now we're ready to project our land-cover raster using the `project()` function from the `rasterio.warp` module. There are a few of things to note about our code here:\n",
    "\n",
    "- We need to open an output file to write data into; we use `rasterio.open()` for this and give it the file mode `'w+'` so that we can both write and read data.\n",
    "- The datatype, or `dtype`, we use in `np.uint8` for 8-bit unsigned integer. \n",
    "- We used `Resampling.mode`, or majority resampling, because the numbers in our land-cover raster represent land-cover categories and shouldn't be interpolated as if they are regular numbers.\n",
    "- The output file, `OUTPUT_LC_FILENAME`, defined at the top of this notebook, will be a GeoTIFF because we gave if the `.tiff` file extension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac2b5be-73b1-4743-8112-dd2f70c79e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See more at: https://rasterio.readthedocs.io/en/stable/topics/resampling.html\n",
    "\n",
    "output_raster = rasterio.open(\n",
    "    OUTPUT_LC_FILENAME, 'w+', count = 1, width = width, height = height,\n",
    "    dtype = np.uint8, crs = pyproj.CRS(6933), transform = new_transform)\n",
    "\n",
    "# Writing the new array to the file\n",
    "reproject(\n",
    "    source = rasterio.band(lc_raster, 1),\n",
    "    destination = rasterio.band(output_raster, 1),\n",
    "    resampling = Resampling.mode,\n",
    "    src_nodata = 255,\n",
    "    dst_nodata = 255)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf569b9-a40f-40ba-b707-618c0c98e2b2",
   "metadata": {},
   "source": [
    "Finally, we convert our land-cover map to a binary croplands map, where 1 indicates croplands are present and 0 otherwise. According to [the MODIS MCD12Q1 User Guide,](https://lpdaac.usgs.gov/documents/1409/MCD12_User_Guide_V61.pdf) in the `\"LC_Type5\"` classification, croplands are originally represented by the number 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88db4f54-3ecf-4527-afb0-651010267371",
   "metadata": {},
   "outputs": [],
   "source": [
    "lc_map_1km = output_raster.read(1)\n",
    "\n",
    "# Create a binary array where Croplands==1, 0 for everything else\n",
    "croplands = np.where(lc_map_1km == 7, 1, 0)\n",
    "pyplot.imshow(croplands, interpolation = 'nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89005c20-4aee-402a-a26d-53779f033f37",
   "metadata": {},
   "source": [
    "Finally, we're going to overwrite the contents of the output GeoTIFF file we just created, replacing the land-cover map with our new Croplands map. We can do this because the necessary data type (and spatial characteristics) of the data are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c2f19e-cd0c-4a22-9e17-c781c01bbbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_raster.write(croplands, 1)\n",
    "output_raster.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "751a3b9b-1f4a-4e2e-985c-6cca4a599b03",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Our first Snakefile\n",
    "\n",
    "Now that we've figured out how to process our land-cover data, let's use Snakemake to generate a rule so that it will be easy to do this again.\n",
    "\n",
    "Let's create a `Snakefile` with the following rule:\n",
    "\n",
    "```\n",
    "rule process_land_cover:\n",
    "    input:\n",
    "        \"data/MCD12Q1/MCD12Q1.A2023001.h18v05.061.2024252125305.hdf\"\n",
    "    output:\n",
    "        \"data/processed/MODIS_MCD12Q1_Type5_cereal_croplands_h15v05_2023.tiff\"\n",
    "    script:\n",
    "        \"workflows/process_land_cover.py\"\n",
    "```\n",
    "\n",
    "This is a high-level description of our land-cover processing task. We can clearly see the input file, the output file, and the Python script that is going to be executed. \n",
    "\n",
    "&#x1F449; **Note that the file paths are relative to the location of the Snakefile.**\n",
    "\n",
    "&#x1F449; Earlier we used the `shell` keyword in a Snakemake rule to execute arbitrary command-line statements. Instead of `shell`, we are now using the `script` keyword to indicate that Snakemake should run a Python script when executing the rule. Snakemake knows the script is a Python script because of the file extension."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f44453-be76-4c97-95fc-aa5180431693",
   "metadata": {},
   "source": [
    "#### &#x1F3C1; Challenge: Writing the land-cover processing script\n",
    "\n",
    "Our Snakemake rule, above, requires that we have a Python script called `process_land_cover.py`. Write this Python script, based on the steps we took above to process the land-cover data. \n",
    "\n",
    "**In your Python script, you can access the `input` and `output` files we specified.** These are known as `snakemake.input` and `snakemake.output`. So, for example, in your Python script you can check to see if the land-cover file has already been downloaded by writing:\n",
    "\n",
    "```python\n",
    "# Only download the files once; i.e., if we haven't already downloaded any\n",
    "if os.path.exists(snakemake.input[0]):\n",
    "    earthaccess.download(results, os.path.dirname(snakemake.input[0]))\n",
    "```\n",
    "\n",
    "**Note that we write `snakemake.input[0]` because there can be multiple inputs!** There can also be multiple outputs. So, when we're ready to create the output raster, we'll write:\n",
    "\n",
    "```python\n",
    "output_raster = rasterio.open(\n",
    "    snakemake.output[0], 'w+', count = 1, width = width, height = height,\n",
    "    dtype = np.uint8, crs = pyproj.CRS(6933), transform = new_transform)\n",
    "```\n",
    "\n",
    "[**Compare your version of `process_land_cover.py` to our script.**](https://github.com/OpenClimateScience/M5-Open-Science-for-Crops/blob/main/notebooks/workflows/process_land_cover.py)\n",
    "\n",
    "#### &#x1F6A9; <span style=\"color:red\">Pay Attention</red>\n",
    "\n",
    "`snakemake.input` and `snakemake.output` are *injected* into our Python code when Snakemake executes the script. **Do not add `import snakemake` to your script. This will not do what you think it does and it will cause an error.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcbe47c-d203-485b-8ead-e5e5980331b4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Downloading ET data\n",
    "\n",
    "[We previously used the Hargreave's equation (Module 2)](https://github.com/OpenClimateScience/M2-Computational-Climate-Science/blob/main/notebooks/04_Processing_Long_Climate_Data_Records.ipynb) to compute PET, but we could also get PET data from NASA's satellite-based ET algorithm, MODIS MOD16, [as we did in Module 3.](https://github.com/OpenClimateScience/M3-Open-Science-for-Water-Resources)\n",
    "\n",
    "In this lesson we'll use PET estimates from [the VIIRS VNP16A2 product](https://dx.doi.org/10.5067/VIIRS/VNP16A2GF.002), which uses the MODIS MOD16 algorithm but is based on data from the newer VIIRS sensor. Both MODIS and VIIRS ET data are available in \"gap-filled\" (GF) versions, so the short name we use for this product is `'VNP16A2GF'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc026cb-80b1-454c-a716-274898cd528c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = earthaccess.search_data(\n",
    "    short_name = 'VNP16A2GF',\n",
    "    temporal = TIME_PERIOD,\n",
    "    bounding_box = tuple(bbox))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e63003f-82f1-4cfa-a535-b01017bbbf85",
   "metadata": {},
   "source": [
    "Like many MODIS and VIIRS data products, VNP16A2 comes in 8-day composites. There are 46 8-day periods in a single year and our study's time period, `TIME_PERIOD`, spans one year. But, when we check the length of `results`, we can see that there are 47 granules. This is because our results include the final 8-day period that starts on the final day of our time series. In general, it's a good idea to check the length of the results from `earthaccess.search_data()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d861b5d-fc89-47a5-9cc3-495515dceb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that we're getting the expected number of granules\n",
    "len(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5e24cb-7db4-4a84-b607-2a39d4750a32",
   "metadata": {},
   "source": [
    "As before, we'll only download the data if they haven't been downloaded before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b75489-1a33-487b-b468-34a661ad89df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only download the files once; i.e., if we haven't already downloaded any\n",
    "if len(glob.glob(f'{VNP16_DIR}/*')) == len(results):\n",
    "    earthaccess.download(results, VNP16_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a060ce8b-c4b7-421d-acb0-8d5d69a235d3",
   "metadata": {},
   "source": [
    "And we'll use `py4eos` again to read the file, even though it is an HDF5 file, because we need an easy way to open the data as a raster dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350ac96d-38be-4ca3-9cee-60a9f9d7d79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf = py4eos.read_file('data/VNP16A2GF/VNP16A2GF.A2024001.h18v05.002.2025021191652.h5', platform = 'VIIRS')\n",
    "hdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0c2fdc-6375-4e3a-b36e-466d016e8721",
   "metadata": {},
   "source": [
    "The file contains two datasets that we're interested in:\n",
    "\n",
    "- `PET_500m` refers to the PET data\n",
    "- `ET_500m` refers to the ET data (which we'll use later)\n",
    "\n",
    "[Based on the User Guide (accessed here),](https://dx.doi.org/10.5067/VIIRS/VNP16A2GF.002) we know that both datasets have a valid range and are scaled to make the file smaller. We use `numpy.where()` to assign `np.nan` to values outside the valid range and to re-scale the valid values by `0.1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b49918f-0c61-4df7-9c93-c379edd2f5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Values greater than or equal to 32700 are NoData values\n",
    "pet0 = hdf.get('PET_500m')\n",
    "pet = np.where(np.abs(et0) >= 32700, np.nan, et0 * 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84cb18ae-ba68-4aa8-8912-d37ac86286dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyplot.imshow(pet, interpolation = 'nearest')\n",
    "pyplot.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c82fab4-2365-4278-841e-c717760f2737",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Resampling, projecting ET and PET data\n",
    "\n",
    "As with the land-cover data, we want to project the ET and PET data onto a 1-km EASE-Grid 2.0. This is slightly more complicated for the ET and PET data, however, as there isn't just a single NoData value; rather, all values greater than 32700 should be considered as invalid values. So, we'll need to use the `read()` method of a `rasterio` dataset to first get a `numpy` array, resampled to 1-km, then write that array into a new `rasterio` dataset before projecting.\n",
    "\n",
    "#### &#x1F3C1; Challenge\n",
    "\n",
    "**Write a function that resamples and then projects the VIIRS ET (or PET) data onto a 1-km EASE-Grid 2.0.** Use the function signature below to get started.\n",
    "\n",
    "```python\n",
    "def reproject_viirs(hdf, field, output_path = '', driver = 'MEM'):\n",
    "    '''\n",
    "    Reprojects a VIIRS ET dataset to the global EASE-Grid 2.0.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    hdf : py4eos.EOSHDF4\n",
    "        The EOSHDF4 instance connected to the VIIRS ET dataset\n",
    "    field : str\n",
    "        The name of the data variable, e.g., \"ET_500m\"\n",
    "    output_path : str\n",
    "        (Optional) The file path, if writing to a file on disk\n",
    "    driver : str\n",
    "        (Optional) The driver name, defaults to \"MEM\"\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    rasterio.io.DatasetWriter\n",
    "    '''\n",
    "    ...\n",
    "```\n",
    "\n",
    "This is a tough problem! See our solution below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf397f4-9c2d-42a4-8dd8-b1d3f3b002e1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def reproject_viirs(hdf, field, output_path = '', driver = 'MEM'):\n",
    "    '''\n",
    "    Reprojects a VIIRS ET dataset to the global EASE-Grid 2.0.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    hdf : py4eos.EOSHDF4\n",
    "        The EOSHDF4 instance connected to the VIIRS ET dataset\n",
    "    field : str\n",
    "        The name of the data variable, e.g., \"ET_500m\"\n",
    "    output_path : str\n",
    "        (Optional) The file path, if writing to a file on disk\n",
    "    driver : str\n",
    "        (Optional) The driver name, defaults to \"MEM\"\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    rasterio.io.DatasetWriter\n",
    "    '''\n",
    "    et_raster = hdf.to_rasterio(\n",
    "        field, filename = '', driver = 'MEM', nodata = 32766., scale_and_offset = True)\n",
    "    \n",
    "    # First, resample the ET data to 1-km resolution\n",
    "    arr = et_raster.read(out_shape = (1200, 1200), resampling = Resampling.average)\n",
    "    arr = np.where(np.abs(arr) >= 32700, np.nan, arr)\n",
    "    # We have to re-create the raster dataset, now at 1-km resolution\n",
    "    et_raster_1km = rasterio.open(\n",
    "        '', 'w+', driver = 'MEM', height = 1200, width = 1200,\n",
    "        count = 1, dtype = np.float32, crs = et_raster.crs, \n",
    "        transform = et_raster.transform * et_raster.transform.scale(2)) # NOTE: Scaling to 1 km\n",
    "    et_raster_1km.write(arr[0], 1)\n",
    "    \n",
    "    # Second, project the data onto a global EASE-Grid 2.0\n",
    "    new_transform, width, height = calculate_default_transform(\n",
    "        et_raster_1km.crs, pyproj.CRS(6933), 1200, 1200, *et_raster_1km.bounds)\n",
    "    et_raster_ease2 = rasterio.open(\n",
    "        output_path, 'w+', driver = driver, height = height, width = width,\n",
    "        count = 1, dtype = np.float32, crs = pyproj.CRS(6933), transform = new_transform)\n",
    "    reproject(\n",
    "        source = rasterio.band(et_raster_1km, 1),\n",
    "        destination = rasterio.band(et_raster_ease2, 1),\n",
    "        resampling = Resampling.bilinear,\n",
    "        src_nodata = np.nan, # Necessary so that missing data is interpolated\n",
    "        dst_nodata = np.nan)\n",
    "    return et_raster_ease2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6c6c6c-93a4-49f5-98f9-d4eb7458d574",
   "metadata": {},
   "source": [
    "Let's preview the ET data from the file we opened up earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177b4d98-eb3f-4b13-812a-8e89c7be2c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "et_raster_ease2 = reproject_viirs(hdf, 'ET_500m')\n",
    "img = et_raster_ease2.read(1)\n",
    "pyplot.imshow(img, interpolation = 'nearest')\n",
    "pyplot.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087821b8-1a6d-4c34-9cf6-15557d2d87b2",
   "metadata": {},
   "source": [
    "Looks good! Now we just need to apply this function to every file we downloaded. We can do that with a `for` loop. We use the `tqdm()` function, from the `tqdm` package, to provide a progress bar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57557a68-2363-4e46-8c34-5308c09b29f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Get a list of all the files\n",
    "file_list = glob.glob(f'{VNP16_DIR}/*')\n",
    "file_list.sort()\n",
    "\n",
    "for filename in tqdm(file_list):\n",
    "    # Read the date from the filename\n",
    "    date = datetime.datetime.strptime(filename.split('/')[-1].split('.')[1], 'A%Y%j')\n",
    "    # Convert the date to a YYYYMMDD string\n",
    "    date_str = date.strftime('%Y%m%d')\n",
    "    # Prepare the output filename\n",
    "    output_file_tpl = f'{OUTPUT_VNP16_DIR}/VNP16_%s_mm_8day-1_{date_str}.tiff'\n",
    "    hdf = py4eos.read_file(filename, platform = 'VIIRS')\n",
    "    et = reproject_viirs(hdf, 'ET_500m', output_file_tpl % 'ET', driver = 'GTiff')\n",
    "    pet = reproject_viirs(hdf, 'PET_500m', output_file_tpl % 'PET', driver = 'GTiff')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de653a1-f82d-4c5c-b8d5-dd61667a8f22",
   "metadata": {},
   "source": [
    "#### &#x1F3AF; Best Practice\n",
    "\n",
    "Note that we called `file_list.sort()` above. Why?\n",
    "\n",
    "It doesn't really matter in this particular case but, in general, **it is often very important to ensure that a list of files is in a certain order before we start processing them.** If the files represent a time series, it may be important to process them in chronological order. Assuming the files have a date string formatted in Year-Month-Day order (e.g., `YYYYMMDD`), another best practice, then a simple call to the file list's `sort()` method will ensure that they are in chronological order.\n",
    "\n",
    "**To emphasize: When dates in filenames are formatter in Year-Month-Day order (e.g., `YYYYMMDD`), then alphanumeric sorting with the `sort()` function is the same as chronological sorting!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ff4f06-0197-4584-aeab-c3f764b40709",
   "metadata": {},
   "source": [
    "## TODO Adding ET data processing to SnakeMake"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d798423f-985f-430e-a626-b78cb3e95b60",
   "metadata": {},
   "source": [
    "- Generalizing input/ output files using `{templates}`\n",
    "- Snakemake will only propose to create output files that already exist\n",
    "- Use of `{wildcards}` in Python commands"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7130a557-c3ea-45ed-8531-27fe62af8aff",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Getting precipitation data from IMERG\n",
    "\n",
    "Next, let's download [precipitation data from IMERG-Final (link)](https://dx.doi.org/10.5067/GPM/IMERGDF/DAY/07)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36407a9-7eec-4650-b97d-5155d4419bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = earthaccess.search_data(\n",
    "    short_name = 'GPM_3IMERGDF',\n",
    "    temporal = TIME_PERIOD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771e3436-956e-4104-b8d8-31676505f781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only download the files once; i.e., if we haven't already downloaded any\n",
    "if len(glob.glob(f'{IMERG_DIR}/*')) == 0:\n",
    "    earthaccess.download(results, IMERG_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca677d5-fbe5-49fc-98c5-19c3a2924796",
   "metadata": {},
   "source": [
    "The IMERG-Final data are in netCDF4 format which, [as we discussed in Module 2,](https://github.com/OpenClimateScience/M2-Computational-Climate-Science) can be handled more easily in Python using `xarray`.\n",
    "\n",
    "The IMERG-Final data also differ from the MODIS land-cover and VIIRS ET data in that each file represents the entire globe. We'll need to crop each file to our study area. An easy way to do this is to use the `bounds` attribute of our `rasterio` dataset, from earlier. We use `shapely.geometry.Polygon` to represent this bounding box in a way that will be understood by the `rioxarray` package. Recall that `rioxarray` is a package built on top of `xarray` that makes it easier to work with spatial datasets in `xarray`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fed971-943e-4578-bc91-c04efc618081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Getting the bounds of our VIIRS tile, for clipping other datasets\n",
    "\n",
    "bb = et_raster_ease2.bounds\n",
    "bounds = Polygon([\n",
    "    (bb.left, bb.bottom), \n",
    "    (bb.left, bb.top),\n",
    "    (bb.right, bb.top),\n",
    "    (bb.right, bb.bottom)\n",
    "])\n",
    "bounds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60958887-f90f-4d1c-a005-1aa5095e5475",
   "metadata": {},
   "source": [
    "Note that the coordinates of our bounding box are in meters easting or meters northing (see below). This is because the coordinate reference system (CRS) of our ET data is now a global EASE-Grid 2.0. We'll need to make sure our precipitation data are also projected onto this same CRS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d01d16c-bcff-4dd4-9c0f-2ee449477d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92dc8fa-0fee-4dc6-9dbd-98222839b574",
   "metadata": {},
   "source": [
    "Remember that when we have `rioxarray` installed, we can access the `.rio` attribute of any `xarray` dataset to use functions that operate on spatial data. For example..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03e1fe0-21ad-433e-9688-6fe379a70ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just an example\n",
    "ds = xr.open_dataset('data/IMERG/3B-DAY.MS.MRG.3IMERG.20231001-S000000-E235959.V07B.nc4')\n",
    "ds.rio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebacc010-dbbb-4424-a902-722562c9bbd2",
   "metadata": {},
   "source": [
    "We're now ready to process the precipitation data from IMERG-Final. Because each IMERG-Final dataset can be opened as an `xarray.Dataset`, our processing takes the form of a series of steps, each one executed by calling a method on the `xarray.Dataset`.\n",
    "\n",
    "1. First, we need to get the dimensions of the `precipitation` DataArray ordered by time, latitude, and longitude. We can do this by calling [the `transpose()` method of a Dataset.](https://docs.xarray.dev/en/stable/generated/xarray.Dataset.transpose.html)\n",
    "2. Next, we need to assign a coordinate reference system (CRS) and tell `xarray` which dimensions correspond to the X and Y coordinate axes of the CRS. We do this with `write_crs()` and `set_spatial_dims()`, respectively. Note that we have to add `.rio` to the beginning of each of these because these functions are part of `rioxarray`.\n",
    "3. Then, we're ready to project the data to a global EASE-Grid 2.0 (EPSG code 6933). For file size considerations, we'll resample them at 9-km resolution. This is done with the `reproject()` method, another `rioxarray` function.\n",
    "4. Finally, we're ready to clip the projected data to the bounds (`bounds`) of our study area using the `clip()` method, another `rioxarray` function.\n",
    "\n",
    "Note that we wrote a `for` loop again, to process each file. We place each projected and clipped dataset into a Python list to store it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e731b39-0b26-4922-80dc-71212010f94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: This may take up to 10 minutes to complete for 1 year's worth of data\n",
    "\n",
    "file_list = glob.glob(f'{IMERG_DIR}/*.nc4')\n",
    "file_list.sort()\n",
    "\n",
    "stack = []\n",
    "for filename in tqdm(file_list):\n",
    "    ds = xr.open_dataset(filename)\n",
    "    # Anytime we use the .rio attribute, we must have rioxarray installed\n",
    "    ds_ease2 = ds[['precipitation']]\\\n",
    "        .transpose('time', 'lat', 'lon')\\\n",
    "        .rio.write_crs(4326)\\\n",
    "        .rio.set_spatial_dims('lon', 'lat')\\\n",
    "        .rio.reproject(pyproj.CRS(6933), resolution = 9000)\\\n",
    "        .rio.clip([bounds])\n",
    "    stack.append(ds_ease2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a2b282-0712-4319-b4fc-d2130042d0dd",
   "metadata": {},
   "source": [
    "#### &#x1F3AF; Best Practice\n",
    "\n",
    "Note that, again, we called `sort()` on the file list, `file_list`. Here, this is essential, because we want the datasets that are put into `stack` to be in chronological order!\n",
    "\n",
    "**As we've seen before, we can combine all of these separate datasets into a single `xarray.Dataset` using the `concat()` method.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbe16b4-d452-49ef-ab32-e8d92cb013a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_precip = xr.concat(stack, dim = 'time')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec19160-3447-493a-9771-744703dce624",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Packaging derived data products\n",
    "\n",
    "`ds_precip` only exists in our computer's memory. Let's not forget to write the dataset to disk! This is another things that's easy to do with `xarray` and, if we write the data to a netCDF4 file, all of the attributes and other metadata will be automatically saved as well!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb3e865-3989-4006-8f6d-e5ddccdce21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_precip.to_netcdf(f'{OUTPUT_IMERG_DIR}/IMERG_precip_mm_day-1_for_study_area.nc4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfaa7bc-337b-47f9-9afd-0b9aa26843b6",
   "metadata": {},
   "source": [
    "I chose to put the units of precipitation (mm per hour) in our filename but, if I had forgotten to do that, note that it is stored as an attribute of the `precipitation` data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa362fa5-a0fb-4220-8701-7019dbfea558",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_precip.precipitation.units"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16f6fef-f9e7-4130-947a-12209b89a642",
   "metadata": {},
   "source": [
    "## TODO Adding precip data processing to SnakeMake"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ScienceCore",
   "language": "python",
   "name": "sciencecore"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
